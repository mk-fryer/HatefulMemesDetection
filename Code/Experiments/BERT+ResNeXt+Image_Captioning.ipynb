{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.7.16 64-bit ('myenv': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"},"colab":{"provenance":[]},"interpreter":{"hash":"147a9a51cec8341ac93583d8be1dd431d4385897d8cf1fc1721a2fb44d714f60"}},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/Abhishek0697/Detection-of-Hate-Speech-in-Multimodal-Memes/blob/main/Code/Experiments/BERT%2BResNeXt%2BImage_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"view-in-github","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","%cd /content/gdrive/MyDrive/HatefulMemesDetection/Code/Experiments"],"metadata":{"id":"9kh_dv1zlIkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"source":["!nvidia-smi"],"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: nvidia-smi: command not found\n"]}],"metadata":{"scrolled":true,"id":"fNxB0trzaP8i","pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682261188953,"user_tz":300,"elapsed":213,"user":{"displayName":"Amber Gupta","userId":"15875376627027560950"}},"outputId":"3c0b93b9-c714-4fdc-86eb-a41611484c8d"}},{"cell_type":"code","source":["!sudo apt-get update -y\n","!sudo apt-get install python3.7\n","!pip install torch transformers torchvision"],"metadata":{"id":"vCYWT6w9lOlv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"source":["!pip install matplotlib"],"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting matplotlib\n","  Downloading matplotlib-3.5.3-cp37-cp37m-macosx_10_9_x86_64.whl (7.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fonttools>=4.22.0\n","  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /Users/awisetraveler/anaconda3/envs/myenv/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n","Collecting kiwisolver>=1.0.1\n","  Downloading kiwisolver-1.4.4-cp37-cp37m-macosx_10_9_x86_64.whl (65 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/awisetraveler/anaconda3/envs/myenv/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /Users/awisetraveler/anaconda3/envs/myenv/lib/python3.7/site-packages (from matplotlib) (21.3)\n","Requirement already satisfied: pyparsing>=2.2.1 in /Users/awisetraveler/anaconda3/envs/myenv/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n","Collecting cycler>=0.10\n","  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n","Requirement already satisfied: pillow>=6.2.0 in /Users/awisetraveler/anaconda3/envs/myenv/lib/python3.7/site-packages (from matplotlib) (9.5.0)\n","Requirement already satisfied: typing-extensions in /Users/awisetraveler/anaconda3/envs/myenv/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.5.0)\n","Requirement already satisfied: six>=1.5 in /Users/awisetraveler/anaconda3/envs/myenv/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Installing collected packages: kiwisolver, fonttools, cycler, matplotlib\n","Successfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.5.3\n"]}],"metadata":{"pycharm":{"name":"#%%\n"},"id":"iA_qbRyNksyG","outputId":"c9ff1ed2-bb22-4518-fcb8-eaf280adeddc"}},{"cell_type":"code","execution_count":null,"source":["'''\n","IMPORTING NECESSARY MODULES\n","'''\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, Dataset, TensorDataset\n","import sys\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils import data\n","import torch.optim as optim\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# sys.path.append('./Trainers/')\n","# sys.path.append('./Dataloaders/')\n","# sys.path.append('./utils/')\n","# sys.path.append('./architectures/')\n","\n","sys.path.append('/content/gdrive/MyDrive/hm_project_pe/data/Trainers/')\n","sys.path.append('/content/gdrive/MyDrive/hm_project_pe/data/Dataloaders/')\n","sys.path.append('/content/gdrive/MyDrive/hm_project_pe/data/utils/')\n","sys.path.append('/content/gdrive/MyDrive/hm_project_pe/data/architectures/')\n","sys.path.append('/content/gdrive/MyDrive/hm_project_pe/data/Test/')\n","\n","from dataloader import mydataset_captioning, mytestdataset\n","from Load_model import load\n","from plot_curves import plot_loss, plot_acc\n","from trainer_Resnet_BERT_Captioning import train, test_classify\n","\n","'''\n","For ResNeXt\n","'''\n","from resnet_models import ResNet,Bottleneck, resnext101_32x8d\n","\n","'''\n","For BERT\n","'''\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup"],"outputs":[],"metadata":{"id":"bAbMlndnaP8k","pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":["**Device**"],"metadata":{"id":"NHfFe7WDaP8k","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"source":["# gpu_ids = [7,6]\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"metadata":{"id":"FFeNpEoAaP8k","pycharm":{"name":"#%%\n"},"outputId":"29fddc35-fe12-49f3-8682-c63f957a2323"}},{"cell_type":"markdown","source":["**Dataloading Scheme**"],"metadata":{"id":"o9tV3N_haP8l","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"source":["trainlist = '/content/gdrive/MyDrive/hm_project_pe/data/train.jsonl'\n","validlist = '/content/gdrive/MyDrive/hm_project_pe/data/dev.jsonl'\n","img_dir = '/content/gdrive/MyDrive/hm_project_pe/data'"],"outputs":[],"metadata":{"id":"uBPb5OqiaP8l","pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"source":["'''\n","Train Dataloader\n","''' \n","train_dataset = mydataset_captioning(annotations_file=trainlist, img_dir=img_dir, name='train')          \n","train_dataloader = data.DataLoader(train_dataset, shuffle= True, batch_size = 32, num_workers=16,pin_memory=True)\n","\n","\n","'''\n","Validation Dataloader\n","''' \n","validation_dataset = mydataset_captioning(annotations_file=validlist, img_dir=img_dir, name='valid')         \n","validation_dataloader = data.DataLoader(validation_dataset, shuffle=False, batch_size = 32, num_workers=16,pin_memory=True)"],"outputs":[],"metadata":{"id":"JYE7ZWHgaP8l","pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":["**Model Definition**"],"metadata":{"id":"DFqdxztfaP8m","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"metadata":{"id":"GeUS8qq6pjPZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"source":["'''\n","Model1 ResNeXt101_32x8d\n","'''\n","# Image_model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes = 2)\n","Image_model = resnext101_32x8d()\n","\n","Image_model.fc = nn.Sequential(\n","    nn.Linear(Image_model.fc.in_features, 2)\n","    )\n","\n","Image_model = nn.DataParallel(Image_model,device_ids = [0]).to(device)\n","\n","\n","'''\n","Load saved model from checkpoint\n","'''\n","model1_name = 'ResneXt101_32x8d'\n","model1_path = './saved_model_checkpoints/'+model1_name\n","\n","checkpoint1 = torch.load(model1_path)\n","Image_model.load_state_dict(checkpoint1['model_state_dict'])\n","Image_model.to(device)"],"outputs":[],"metadata":{"id":"BxRnFreKaP8n","pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"source":["'''\n","Model 2 BERT\n","\n","Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n","''' \n","\n","Text_model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", \n","    num_labels = 2,   \n","    output_attentions = False, \n","    output_hidden_states = True\n",")\n","\n","Text_model = nn.DataParallel(Text_model,device_ids=[0]).to(device)\n","\n","\n","'''\n","Load saved model from checkpoint\n","'''\n","model2_name = 'BERT_basic'\n","model2_path = './saved_model_checkpoints/'+model2_name\n","\n","checkpoint2 = torch.load(model2_path)\n","Text_model.load_state_dict(checkpoint2['model_state_dict'])\n","\n","Text_model.to(device)"],"outputs":[],"metadata":{"id":"8AyJYW6haP8n","pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"source":["'''\n","Fusion \n","Image Features, Text Features and Captions generated by our Captioning model\n","'''\n","class FusionNet(nn.Module):\n","    \n","    def __init__(self, num_classes, drop_prob = 0.1):\n","        super(FusionNet, self).__init__()\n","        \n","        self.concat = nn.Linear(in_features=768+2048+768, out_features= 512)\n","        \n","        self.bn = nn.BatchNorm1d(512)\n","        self.bn1 = nn.BatchNorm1d(768)\n","        self.bn2 = nn.BatchNorm1d(2048)\n","        self.bn3 = nn.BatchNorm1d(768)\n","\n","        \n","        self.dropout = nn.Dropout(drop_prob)\n","        \n","        self.classify = nn.Linear(in_features = 512, out_features = num_classes)\n","        \n","        \n","    def forward(self, text_features, image_features, caption_features):\n","\n","        text_features = self.bn1(text_features)\n","        image_features = self.bn2(image_features)\n","        caption_features = self.bn3(caption_features)\n","\n","        fused_input =  torch.cat((text_features, image_features, caption_features), dim=1)\n","        \n","        x = self.concat(fused_input)\n","        x = F.relu(self.bn(x))        \n","        \n","        x = F.relu(self.classify(x)) \n","\n","        return x\n"],"outputs":[],"metadata":{"id":"c3VLQHnuaP8o","pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"source":["Fusion_model = FusionNet(num_classes = 2 , drop_prob = 0.1)\n","Fusion_model = nn.DataParallel(Fusion_model, device_ids=[0]).to(device)"],"outputs":[],"metadata":{"id":"85pRXktUaP8o","pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":["**Hyperparameters**"],"metadata":{"id":"Kdt6fL6-aP8o","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"source":["'''\n","Loss Function\n","'''\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","'''\n","Optimizer\n","'''\n","optimizer = torch.optim.SGD(Fusion_model.parameters(), lr=0.1, weight_decay=1e-4, momentum=0.9)\n","# optimizer = AdamW(Fusion_model.parameters(), lr = 2e-3, eps = 1e-8)\n","\n","\n","'''\n","Number of training epochs.\n","'''\n","num_Epochs = 10\n","\n","\n","# '''\n","# OneCycleLR\n","# '''\n","# max_lr = 0.05\n","# lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None, epochs=num_Epochs, steps_per_epoch=len(train_dataloader), pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, last_epoch=-1)\n","\n","\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 4, gamma = 0.001)\n","\n","# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)"],"outputs":[],"metadata":{"id":"KC_FZ5uCaP8o","pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"source":["model_name = 'ImageCaptioning'\n","model_path = '.saved_model_checkpoints/'+model_name"],"outputs":[],"metadata":{"id":"agClo63KaP8p","pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"source":["writer = SummaryWriter(model_name)\n","\n","train(Image_model, Text_model, Fusion_model, train_dataloader, validation_dataloader, criterion, optimizer, lr_scheduler, model_path, writer, device, epochs = num_Epochs)\n","\n","writer.flush()\n","writer.close()"],"outputs":[],"metadata":{"id":"QJUzH2BIaP8p","pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"source":["'''\n","Load saved model from checkpoint\n","'''\n","Fusion_model, optimizer, lr_scheduler, train_loss, v_loss, v_acc, epoch = load(model_path, Fusion_model, optimizer, lr_scheduler)\n"],"outputs":[],"metadata":{"id":"yL0-sKbkaP8p","pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":["**Evaluate**"],"metadata":{"id":"EZY-h3UtaP8q","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"source":["test_classify(Fusion_model, validation_dataloader, criterion, device)"],"outputs":[],"metadata":{"id":"4LZVVQ-1aP8q","pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":["**Predict on Test and generate output.csv**"],"metadata":{"id":"fzTSBs4SaP8q","pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":["**Test Dataloader**"],"metadata":{"id":"uQM4UyDkaP8r","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"source":["testlist = 'path of test set'\n","\n","test_dataset = mytestdataset(testlist, name='test')          \n","test_dataloader = data.DataLoader(test_dataset, shuffle= False, batch_size = 32, num_workers=8,pin_memory=True)"],"outputs":[],"metadata":{"id":"fHDvADPBaP8r","pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"source":["from predict import predict\n","predict(image_model, text_model, fusion_model, test_dataloader, device)"],"outputs":[],"metadata":{"id":"zPoGZ1L0aP8r","pycharm":{"name":"#%%\n"}}}]}